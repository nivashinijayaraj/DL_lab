{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Basic Entailment Using simple Rule Based Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T15:54:17.662685Z",
     "iopub.status.busy": "2024-10-05T15:54:17.662281Z",
     "iopub.status.idle": "2024-10-05T15:54:20.770420Z",
     "shell.execute_reply": "2024-10-05T15:54:20.769229Z",
     "shell.execute_reply.started": "2024-10-05T15:54:17.662647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Accuracy: 0.33876719307182884\n",
      "Entailment: False\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download necessary resources for NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "# Assume the dataset is in the form of a CSV file containing two sentences and a label ('entailment' or 'non-entailment')\n",
    "# Replace 'text_entailment_dataset.csv' with the path to your dataset\n",
    "data = pd.read_csv('/kaggle/input/textual-entailment-dataset/validation.csv')\n",
    "\n",
    "# Step 3: Preprocess the Data\n",
    "# Function to preprocess text by tokenizing and converting to lowercase\n",
    "def preprocess(text):\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Apply preprocessing to both sentences in each pair\n",
    "data['sentence1_tokens'] = data['text1'].apply(preprocess)\n",
    "data['sentence2_tokens'] = data['text2'].apply(preprocess)\n",
    "\n",
    "# Step 4: Define Simple Rule-Based Method\n",
    "# Function to check if all tokens in sentence2 are in sentence1\n",
    "def simple_rule_based_entailment(s1, s2):\n",
    "    return set(s2).issubset(set(s1))\n",
    "\n",
    "# Apply the rule-based method to each sentence pair\n",
    "data['prediction'] = data.apply(lambda row: simple_rule_based_entailment(row['sentence1_tokens'], row['sentence2_tokens']), axis=1)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "# Calculate the accuracy of the predictions\n",
    "accuracy = accuracy_score(data['label'], data['prediction'])\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "# Sample Input:\n",
    "sentence1 = \"The cat is on the mat.\"\n",
    "sentence2 = \"The mat has a cat.\"\n",
    "\n",
    "# Preprocess the sample input\n",
    "sentence1_tokens = preprocess(sentence1)\n",
    "sentence2_tokens = preprocess(sentence2)\n",
    "\n",
    "# Check if entailment is predicted\n",
    "is_entailment = simple_rule_based_entailment(sentence1_tokens, sentence2_tokens)\n",
    "print(f'Entailment: {is_entailment}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Natural Language Inference with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T17:04:04.257513Z",
     "iopub.status.busy": "2024-10-05T17:04:04.256983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}\n",
      "{'premise': ['A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'Children smiling and waving at camera', 'Children smiling and waving at camera'], 'hypothesis': ['A person is training his horse for a competition.', 'A person is at a diner, ordering an omelette.', 'A person is outdoors, on a horse.', 'They are smiling at their parents', 'There are children present'], 'label': [1, 2, 0, 1, 0]}\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "})\n",
      "Label examples:\n",
      "[1, 2, 0, 1, 0]\n",
      "Unique labels in the dataset: {0, 1, 2, -1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b17b2e4d71d4dec9041a1380580dce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857a678713f44504a2db27d0ffe3ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169a20d829eb4574972904c1fa41de04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/550152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b6d7732c0d425980747141b09ca799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Step 2: Load the Dataset\n",
    "dataset = load_dataset('snli')\n",
    "\n",
    "# Check the first few examples to understand the structure\n",
    "print(dataset['train'].features)  # Check the features of the training dataset\n",
    "print(dataset['train'][0:5])       # Print the first 5 examples from the training dataset\n",
    "\n",
    "# Step 3: Preprocess the Data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Apply preprocessing to the dataset (train, validation, and test splits)\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Check the structure of the dataset again\n",
    "print(encoded_dataset)\n",
    "\n",
    "# Step 4: Inspect the label column directly to understand its structure\n",
    "print(\"Label examples:\")\n",
    "print(encoded_dataset['train']['label'][0:5])  # Print the first 5 labels\n",
    "\n",
    "# Step 5: Identify unique labels\n",
    "unique_labels = set(encoded_dataset['train']['label'])\n",
    "print(f\"Unique labels in the dataset: {unique_labels}\")\n",
    "\n",
    "# Step 6: Define label mapping and handle unexpected labels\n",
    "label_dict = {0: 0, 1: 1, 2: 2}  # Adjust this as necessary based on your labels\n",
    "\n",
    "# Step 7: Map the labels correctly, handle unexpected labels\n",
    "def map_labels(example):\n",
    "    # Use the label_dict for mapping, and set a default for unexpected labels\n",
    "    label = example['label']\n",
    "    return {'labels': label_dict.get(label, -1)}  # Map to -1 if the label is unexpected\n",
    "\n",
    "encoded_dataset = encoded_dataset.map(map_labels)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Step 8: Load the Pre-Trained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Step 9: Set Up Training Arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    evaluation_strategy='epoch',     # Evaluation during each epoch\n",
    "    per_device_train_batch_size=16,  # Batch size for training\n",
    "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
    "    num_train_epochs=3,              # Number of training epochs\n",
    "    weight_decay=0.01,               # Strength of L2 regularization\n",
    "    logging_dir='./logs',            # Directory for logs\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the model, training arguments, and datasets\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The BERT model for training\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=encoded_dataset['train'],  # Training dataset\n",
    "    eval_dataset=encoded_dataset['validation'],  # Validation dataset\n",
    ")\n",
    "\n",
    "# Step 10: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 11: Evaluate the Model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")\n",
    "\n",
    "# Step 12: Make Predictions\n",
    "premise = \"A man inspects the uniform of a figure in some East Asian country.\"\n",
    "hypothesis = \"The man is sleeping.\"\n",
    "\n",
    "# Tokenize the input example\n",
    "inputs = tokenizer(premise, hypothesis, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Get model prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predicted_label = torch.argmax(outputs.logits).item()\n",
    "\n",
    "# Convert prediction to human-readable label\n",
    "label_map = {0: 'entailment', 1: 'contradiction', 2: 'neutral'}\n",
    "print(f\"Predicted Label: {label_map[predicted_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C) Sentence Pair Classification Using Siamese Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Preprocess the Data: Load dataset and generate embeddings\n",
    "data = pd.read_csv('sentence_similarity_dataset.csv')\n",
    "\n",
    "# Load a pre-trained Sentence Transformer model for generating sentence embeddings\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Get sentence embeddings\n",
    "data['sentence1_embedding'] = data['sentence1'].apply(lambda x: model.encode(x))\n",
    "data['sentence2_embedding'] = data['sentence2'].apply(lambda x: model.encode(x))\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "X1 = np.array(data['sentence1_embedding'].tolist())\n",
    "X2 = np.array(data['sentence2_embedding'].tolist())\n",
    "y = data['similarity_label'].values  # Assuming the dataset has a column for similarity labels\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X1_train, X1_test, X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define the Siamese Network Architecture\n",
    "# Input layers for the two sentences\n",
    "input1 = layers.Input(shape=(X1.shape[1],))\n",
    "input2 = layers.Input(shape=(X1.shape[1],))\n",
    "\n",
    "# Dense layers for feature extraction\n",
    "dense_layer = layers.Dense(128, activation='relu')\n",
    "encoded1 = dense_layer(input1)\n",
    "encoded2 = dense_layer(input2)\n",
    "\n",
    "# Concatenate the extracted features and add a final dense layer for output\n",
    "merged = layers.concatenate([encoded1, encoded2])\n",
    "output = layers.Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "# Define the Siamese model\n",
    "siamese_model = models.Model(inputs=[input1, input2], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "siamese_model.compile(optimizer=optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Train the model\n",
    "siamese_model.fit([X1_train, X2_train], y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# 4. Evaluate the Model\n",
    "# Predict on the test set\n",
    "y_pred = siamese_model.predict([X1_test, X2_test])\n",
    "\n",
    "# Convert predictions to binary (similar or not similar)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 950271,
     "sourceId": 1628138,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4115102,
     "sourceId": 7132273,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
