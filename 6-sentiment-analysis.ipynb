{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A)Basic Sentiment Analysis using Logistic Regressing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:15:33.112490Z",
     "iopub.status.busy": "2024-09-25T12:15:33.111749Z",
     "iopub.status.idle": "2024-09-25T12:15:49.447495Z",
     "shell.execute_reply": "2024-09-25T12:15:49.446351Z",
     "shell.execute_reply.started": "2024-09-25T12:15:33.112432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                             review\n",
      "0      0  Once again Mr. Costner has dragged out a movie...\n",
      "1      0  This is an example of why the majority of acti...\n",
      "2      0  First of all I hate those moronic rappers, who...\n",
      "3      0  Not even the Beatles could write songs everyon...\n",
      "4      0  Brass pictures (movies is not a fitting word f...\n",
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      5022\n",
      "           1       0.88      0.90      0.89      4978\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Sample Review Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "#a\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Replace 'path_to_dataset/reviews.csv' with the actual path to the IMDb dataset.\n",
    "df = pd.read_csv('/kaggle/input/movie-review/labelled_full_dataset.csv')\n",
    "\n",
    "# Check the structure of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove digits\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'review' column\n",
    "df['cleaned_text'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Step 3: Convert text data into numerical vectors\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Use TF-IDF for vectorization\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "y = df['label']  # Use the 'label' column for sentiment labels\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 5: Build and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Sample Input (Review Text)\n",
    "sample_review = \"This movie was amazing! I loved every minute of it.\"\n",
    "sample_cleaned = clean_text(sample_review)\n",
    "sample_vectorized = vectorizer.transform([sample_cleaned])\n",
    "\n",
    "# Predict sentiment for the sample input\n",
    "sample_prediction = model.predict(sample_vectorized)\n",
    "print(f'Sample Review Sentiment: {\"Positive\" if sample_prediction[0] == 1 else \"Negative\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Twitter Sentiment Analysis Using LSTM and Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T12:16:07.198550Z",
     "iopub.status.busy": "2024-09-25T12:16:07.197753Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ItemID  Sentiment SentimentSource  \\\n",
      "0       1          0    Sentiment140   \n",
      "1       2          0    Sentiment140   \n",
      "2       3          1    Sentiment140   \n",
      "3       4          0    Sentiment140   \n",
      "4       5          0    Sentiment140   \n",
      "\n",
      "                                       SentimentText  \n",
      "0                       is so sad for my APL frie...  \n",
      "1                     I missed the New Moon trail...  \n",
      "2                            omg its already 7:30 :O  \n",
      "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
      "4           i think mi bf is cheating on me!!!   ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m19111/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2:02\u001b[0m 197ms/step - accuracy: 0.4987 - loss: 0.6969"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/kaggle/input/twitter-sentiment/Sentiment Analysis Dataset 2.csv', on_bad_lines='skip')\n",
    "\n",
    "# Check the structure of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'SentimentText' column\n",
    "df['cleaned_text'] = df['SentimentText'].apply(clean_text)\n",
    "\n",
    "# Convert sentiment labels to numerical format\n",
    "df['Sentiment'] = df['Sentiment'].replace({0: 0, 2: 1, 4: 2})  # Adjust based on your labeling scheme\n",
    "\n",
    "# Step 2: Tokenize text and pad sequences\n",
    "max_length = 100  # Maximum length of sequences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['cleaned_text'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Step 3: Split data into training, validation, and testing sets\n",
    "X = padded_sequences\n",
    "y = df['Sentiment'].values\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split the temp set into validation and test sets\n",
    "\n",
    "# Step 4: Load pre-trained GloVe embeddings\n",
    "embeddings_index = {}\n",
    "glove_file = '/kaggle/input/glove-embeddings/glove.6B.100d.txt'  # Adjust the path to your GloVe file\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Step 5: Build LSTM model with GloVe embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n",
    "                    output_dim=embedding_dim, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=max_length, \n",
    "                    trainable=False))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes (0, 1, 2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model with validation data\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, \n",
    "                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Sample Input (Tweet Text)\n",
    "sample_tweet = \"I really enjoyed the movie, it was fantastic!\"\n",
    "sample_cleaned = clean_text(sample_tweet)\n",
    "sample_sequence = tokenizer.texts_to_sequences([sample_cleaned])\n",
    "sample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "# Predict sentiment for the sample input\n",
    "sample_prediction = model.predict(sample_padded)\n",
    "print(f'Sample Tweet Sentiment: {np.argmax(sample_prediction)}')  # Output the sentiment class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-25T10:13:48.923631Z",
     "iopub.status.busy": "2024-09-25T10:13:48.923145Z",
     "iopub.status.idle": "2024-09-25T10:13:48.930518Z",
     "shell.execute_reply": "2024-09-25T10:13:48.929168Z",
     "shell.execute_reply.started": "2024-09-25T10:13:48.923590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using-word-embeddings-for-sentiment-analysis', 'imdb-review', 'twitter-sentiment', 'movie-review', 'twitter-entity-sentiment-analysis', 'd', 'twitter-sentiment-analysis-using-tensorflow']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('/kaggle/input/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Movie Reviews Sentiment Classification with Bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-25T11:16:48.115977Z",
     "iopub.status.idle": "2024-09-25T11:16:48.116440Z",
     "shell.execute_reply": "2024-09-25T11:16:48.116248Z",
     "shell.execute_reply.started": "2024-09-25T11:16:48.116225Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "# Load the dataset \n",
    "df = pd.read_csv('/kaggle/input/imdb-review/imdb_reviews.csv')  # Adjust to your actual dataset path\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Convert sentiment labels to numerical format (0 for negative, 1 for positive)\n",
    "df['sentiment'] = df['sentiment'].map({'neg': 0, 'pos': 1})\n",
    "\n",
    "# Clean text function (optional, based on dataset specifics)\n",
    "def clean_text(text):\n",
    "    # Here, you can add any cleaning steps if necessary (e.g., removing special characters)\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Step 2: Tokenize and encode reviews using BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, reviews, labels):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize and encode the review\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Step 3: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = IMDBDataset(X_train.to_numpy(), y_train.to_numpy())\n",
    "test_dataset = IMDBDataset(X_test.to_numpy(), y_test.to_numpy())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Step 4: Load pre-trained BERT model and fine-tune for sentiment classification\n",
    "# Load tokenizer from a local directory where BERT files are stored\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('/kaggle/input/bert-base-uncased', num_labels=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(3):  # Adjust number of epochs as necessary\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{3}, Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "# Calculate accuracy and classification report\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Sample Input\n",
    "sample_review = \"The movie was boring and uninteresting.\"\n",
    "encoded_sample = tokenizer.encode_plus(\n",
    "    sample_review,\n",
    "    add_special_tokens=True,\n",
    "    max_length=256,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "# Make prediction on the sample input\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_ids = encoded_sample['input_ids'].to(device)\n",
    "    attention_mask = encoded_sample['attention_mask'].to(device)\n",
    "    output = model(input_ids, attention_mask=attention_mask)\n",
    "    prediction = torch.argmax(output.logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(f'Sample Input: \"{sample_review}\"')\n",
    "print(f'Expected Output: {\"Positive\" if prediction[0] == 1 else \"Negative\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5689,
     "sourceId": 8482,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 73247,
     "sourceId": 163621,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1209510,
     "sourceId": 2020792,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1520310,
     "sourceId": 2510329,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5764856,
     "sourceId": 9478143,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 2859008,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 55281833,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
