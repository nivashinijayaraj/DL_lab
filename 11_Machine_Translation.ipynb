{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.  Basic Machine Translation uisng Rule- Based Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: bonjour monde\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an English sentence to translate (or type 'exit' to quit):  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: bonjour\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an English sentence to translate (or type 'exit' to quit):  am\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentence: suis\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter an English sentence to translate (or type 'exit' to quit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting translation system.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the Bilingual Dictionary\n",
    "dictionary = {\n",
    "    'hello': 'bonjour',\n",
    "    'world': 'monde',\n",
    "    'my': 'mon',\n",
    "    'name': 'nom',\n",
    "    'is': 'est',\n",
    "    'good': 'bon',\n",
    "    'morning': 'matin',\n",
    "    'i': 'je',\n",
    "    'am': 'suis',\n",
    "    'a': 'un',\n",
    "    'student': 'Ã©tudiant',\n",
    "    'teacher': 'professeur'\n",
    "}\n",
    "\n",
    "# Step 2: Define Grammar Rules\n",
    "grammar_rules = {\n",
    "    'SVO': ['subject', 'verb', 'object']  # Subject-Verb-Object structure\n",
    "}\n",
    "\n",
    "# Step 3: Translation Function\n",
    "def translate(sentence):\n",
    "    # Convert sentence to lowercase and split into words\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    # Translate each word using the dictionary\n",
    "    translated_words = [dictionary.get(word, word) for word in words]\n",
    "    \n",
    "    # Join the translated words into a sentence\n",
    "    translated_sentence = ' '.join(translated_words)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Hello world\"\n",
    "translated_sentence = translate(sentence)\n",
    "print(\"Translated sentence:\", translated_sentence)\n",
    "\n",
    "# Sample input/output interaction\n",
    "while True:\n",
    "    user_input = input(\"Enter an English sentence to translate (or type 'exit' to quit): \")\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting translation system.\")\n",
    "        break\n",
    "    \n",
    "    translated_output = translate(user_input)\n",
    "    print(\"Translated sentence:\", translated_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B. English to French Translation Using Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T07:35:23.797669Z",
     "iopub.status.busy": "2024-10-06T07:35:23.797228Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Step 1: Load dataset from CSV using Pandas\n",
    "data_path = '/kaggle/input/en-fr-translation-dataset/en-fr.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check the first few rows and the column names of the dataframe\n",
    "print(data.head())\n",
    "print(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n",
    "\n",
    "# Ensure the dataframe contains the required columns\n",
    "expected_columns = ['en', 'fr']\n",
    "assert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n",
    "\n",
    "# Step 2: Convert the DataFrame to a TensorFlow Dataset\n",
    "# Create a TensorFlow dataset from the DataFrame\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n",
    "\n",
    "# Print the first example to verify conversion\n",
    "for english, french in train_dataset.take(1):\n",
    "    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n",
    "\n",
    "# Optional: Define constants for batch size and max length\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "# Optional: Tokenization process\n",
    "# Tokenizer setup for input (English) and output (French)\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n",
    "tokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n",
    "\n",
    "# Encoding function\n",
    "def encode(en_t, fr_t):\n",
    "    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n",
    "    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n",
    "    return en_t, fr_t\n",
    "\n",
    "def tf_encode(en_t, fr_t):\n",
    "    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n",
    "\n",
    "# Prepare the dataset with encoding\n",
    "train_dataset = train_dataset.map(tf_encode)\n",
    "\n",
    "# Filter sequences longer than MAX_LENGTH\n",
    "def filter_max_length(en, fr, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n",
    "\n",
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "train_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Print the first training example after processing\n",
    "for en, fr in train_dataset.take(1):\n",
    "    print(f'Encoded English: {en.numpy()}')\n",
    "    print(f'Encoded French: {fr.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) NEURAL MACHINE TRANSLATION WITH TRANSFORMERS (ENGLISH TO GERMAN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Load the dataset\n",
    "dataset, metadata = tfds.load('wmt14_translate/de-en', with_info=True, as_supervised=True)\n",
    "train_examples, val_examples = dataset['train'], dataset['validation']\n",
    "\n",
    "# Tokenizer and preprocessing\n",
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (en.numpy() for en, de in train_examples), target_vocab_size=2**13)\n",
    "tokenizer_de = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (de.numpy() for en, de in train_examples), target_vocab_size=2**13)\n",
    "\n",
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang1.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    lang2 = [tokenizer_de.vocab_size] + tokenizer_de.encode(lang2.numpy()) + [tokenizer_de.vocab_size+1]\n",
    "    return lang1, lang2\n",
    "\n",
    "def tf_encode(en, de):\n",
    "    result_en, result_de = tf.py_function(encode, [en, de], [tf.int64, tf.int64])\n",
    "    result_en.set_shape([None])\n",
    "    result_de.set_shape([None])\n",
    "    return result_en, result_de\n",
    "\n",
    "train_dataset = train_examples.map(tf_encode).padded_batch(64, padded_shapes=([None], [None]))\n",
    "val_dataset = val_examples.map(tf_encode).padded_batch(64, padded_shapes=([None], [None]))\n",
    "\n",
    "# Transformer Model\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inputs, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inputs[0], training, enc_padding_mask)\n",
    "        dec_output, _ = self.decoder(inputs[1], enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "# Define hyperparameters\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "input_vocab_size = tokenizer_en.vocab_size + 2\n",
    "target_vocab_size = tokenizer_de.vocab_size + 2\n",
    "pe_input = 1000\n",
    "pe_target = 1000\n",
    "\n",
    "# Instantiate and compile model\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, dropout_rate)\n",
    "\n",
    "learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001, decay_steps=10000, decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "# Training loop\n",
    "transformer.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Sample translation\n",
    "sample_sentence = \"Hello, how are you?\"\n",
    "sample_input = tokenizer_en.encode(sample_sentence)\n",
    "sample_input = tf.expand_dims(sample_input, axis=0)\n",
    "\n",
    "# Prediction\n",
    "output = transformer.predict([sample_input, tf.zeros_like(sample_input)])\n",
    "output_sentence = tokenizer_de.decode([int(i) for i in tf.argmax(output, axis=-1)[0] if i < tokenizer_de.vocab_size])\n",
    "\n",
    "print(f\"Input: {sample_sentence}\")\n",
    "print(f\"Translation: {output_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1148896,
     "sourceId": 1926230,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
