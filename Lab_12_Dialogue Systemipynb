{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Basic Rule Based Chatbot using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi, I'm the chatbot you created. Type 'quit' to exit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> what is your name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a bot .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "> quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye! Take care.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Import Libraries\n",
    "import nltk\n",
    "from nltk.chat.util import Chat, reflections\n",
    "\n",
    "# Step 3: Define Rules (Predefined pairs)\n",
    "pairs = [\n",
    "    (r\"my name is (.*)\", [\"Hello %1, How are you today?\"]),\n",
    "    (r\"hi|hey|hello\", [\"Hello\", \"Hey there\"]),\n",
    "    (r\"what is your name?\", [\"I am a bot .\"]),\n",
    "    (r\"how are you?\", [\"I'm doing good. How about you?\"]),\n",
    "    (r\"sorry (.*)\", [\"No problem\", \"It's okay\", \"You don't need to be sorry\"]),\n",
    "    (r\"quit\", [\"Bye! Take care.\"])\n",
    "]\n",
    "\n",
    "# Step 4: Create the Chatbot\n",
    "def chatbot():\n",
    "    print(\"Hi, I'm the chatbot you created. Type 'quit' to exit.\") \n",
    "    chat = Chat(pairs, reflections)\n",
    "    chat.converse()\n",
    "    \n",
    "# Step 5: Run the Chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B] Building a chatbot Using seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T05:38:41.575203Z",
     "iopub.status.busy": "2024-10-06T05:38:41.574709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 304446 conversations.\n",
      "Created 304445 input-target pairs.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Step 1: Load and Preprocess the Dataset\n",
    "def load_data(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return []\n",
    "\n",
    "    conversations = []\n",
    "    for line in lines:\n",
    "        line_parts = line.strip().split(' +++$+++ ')\n",
    "        if len(line_parts) == 5:\n",
    "            conversations.append(line_parts[4])  # Store only the dialogue part\n",
    "\n",
    "    print(f\"Loaded {len(conversations)} conversations.\")  # Debug info\n",
    "    return conversations\n",
    "\n",
    "def create_pairs(conversations):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for i in range(len(conversations) - 1):\n",
    "        input_text = conversations[i]\n",
    "        target_text = conversations[i + 1]\n",
    "        target_text = '\\t' + target_text + '\\n'  # Add start and end tokens\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "    print(f\"Created {len(input_texts)} input-target pairs.\")  # Debug info\n",
    "    return input_texts, target_texts\n",
    "\n",
    "# Load the dataset (replace with the correct path to movie_lines.txt)\n",
    "conversations = load_data('/kaggle/input/movie-dialogs/movie_lines.txt')  # Make sure this file exists\n",
    "input_texts, target_texts = create_pairs(conversations)\n",
    "\n",
    "# Check if input_texts and target_texts are populated\n",
    "if not input_texts or not target_texts:\n",
    "    raise ValueError(\"No input or target texts were created. Please check the dataset.\")\n",
    "\n",
    "# Step 2: Tokenize and Pad the Data\n",
    "# Tokenize the input and output data\n",
    "input_tokenizer = Tokenizer()\n",
    "target_tokenizer = Tokenizer()\n",
    "\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "target_tokenizer.fit_on_texts(target_texts)\n",
    "\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "max_encoder_seq_length = max(len(seq) for seq in input_sequences) if input_sequences else 0\n",
    "max_decoder_seq_length = max(len(seq) for seq in target_sequences) if target_sequences else 0\n",
    "\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "# Prepare decoder output data\n",
    "decoder_output_data = np.zeros((len(target_sequences), max_decoder_seq_length, len(target_tokenizer.word_index) + 1), dtype='float32')\n",
    "\n",
    "for i, seq in enumerate(target_sequences):\n",
    "    for t, word_idx in enumerate(seq):\n",
    "        if t > 0:\n",
    "            decoder_output_data[i, t - 1, word_idx] = 1.0\n",
    "\n",
    "# Step 3: Build the Seq2Seq Model\n",
    "num_encoder_tokens = len(input_tokenizer.word_index) + 1\n",
    "num_decoder_tokens = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Save the encoder states to pass to the decoder\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Step 4: Compile and Train the Model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model (adjust epochs and batch size as needed)\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=100)\n",
    "\n",
    "# Step 5: Inference Setup (for generating responses)\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder setup\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Step 6: Decode a Sequence (Generate a Response)\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate an empty target sequence with only the start token\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample the next token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence and states\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence.strip()  # Trim any extra whitespace\n",
    "\n",
    "# Step 7: Test the Chatbot\n",
    "def chat():\n",
    "    print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        input_text = input(\"You: \")\n",
    "        if input_text.lower() == 'quit':\n",
    "            print(\"Exiting the chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "        input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n",
    "        response = decode_sequence(input_sequence)\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Section c: Conversational AI with Transformer-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(prompt):\n",
    "    # Encode the input and generate tokens\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate response using the model\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=100, num_return_sequences=1, \n",
    "                                no_repeat_ngram_size=2, top_p=0.95, temperature=0.7)\n",
    "\n",
    "    # Decode the generated tokens to text\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generating Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample conversation\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # Exit condition\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Generate and print the response\n",
    "    response = generate_response(user_input)\n",
    "    print(f\"Bot: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 850118,
     "sourceId": 1450246,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
