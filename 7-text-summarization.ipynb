{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2734496,"sourceType":"datasetVersion","datasetId":1654566}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"A. BASIC TEXT SUMMARIZATION USING TF-IDF AND COSINE SIMILARITY","metadata":{}},{"cell_type":"code","source":"# Step 1: Import Required Libraries\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Download necessary resources for nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Step 2: Define Sample Text\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \nconcerned with the interactions between computers and human language, in particular how to program computers \nto process and analyze large amounts of natural language data. \nChallenges in natural language processing frequently involve speech recognition, \nnatural language understanding, and natural language generation.\n\"\"\"\n\n# Step 3: Preprocess the Text\n# Split text into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Define stopwords\nstop_words = set(stopwords.words('english'))\n\n# Function to remove stopwords\ndef preprocess_sentence(sentence):\n    return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n\n# Apply preprocessing to each sentence\npreprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n\n# Step 4: Compute TF-IDF Matrix\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n\n# Step 5: Compute Cosine Similarity\ncosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# Step 6: Generate Summary\ndef generate_summary(sentences, sim_matrix, top_n=2):\n    # Sum of similarity scores for each sentence\n    scores = sim_matrix.sum(axis=1)\n    # Get the top-ranked sentences based on similarity scores\n    ranked_sentences = [sentences[i] for i in scores.argsort()[-top_n:]]\n    return ' '.join(ranked_sentences)\n\n# Generate and print the summary\nsummary = generate_summary(sentences, cosine_sim_matrix)\nprint(\"Summary:\")\nprint(summary)\n","metadata":{"execution":{"iopub.execute_input":"2024-10-05T15:42:21.452202Z","iopub.status.busy":"2024-10-05T15:42:21.451304Z","iopub.status.idle":"2024-10-05T15:42:22.636971Z","shell.execute_reply":"2024-10-05T15:42:22.635992Z","shell.execute_reply.started":"2024-10-05T15:42:21.452157Z"}},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n\n[nltk_data]   Package punkt is already up-to-date!\n\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n\n[nltk_data]   Package stopwords is already up-to-date!\n\nSummary:\n\n\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \n\nconcerned with the interactions between computers and human language, in particular how to program computers \n\nto process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve speech recognition, \n\nnatural language understanding, and natural language generation.\n"}]},{"cell_type":"markdown","source":"B. ABSTRACTIVE TEXT SUMMARIZATION WITH TRANSFORMERS","metadata":{}},{"cell_type":"code","source":"# Import required libraries\nfrom transformers import BartForConditionalGeneration, BartTokenizer\nfrom datasets import load_dataset\n\n# Load the dataset (using 1% of the test split of CNN/DailyMail)\ndataset = load_dataset(\"/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail\", split=\"test[:1%]\")\n\n# Load pre-trained BART model and tokenizer\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n\n# Function to generate a summary using the BART model\ndef summarize(text):\n    # Tokenize input text\n    inputs = tokenizer([text], max_length=1024, return_tensors='pt', truncation=True)\n    \n    # Generate summary ids\n    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n    \n    # Decode the summary and return\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Pick an article from the dataset\narticle = dataset[0]['article']\n\n# Generate summary\nsummary = summarize(article)\n\n# Print the original article and the generated summary\nprint(\"Original Article:\\n\", article)\nprint(\"\\nGenerated Summary:\\n\", summary)","metadata":{"execution":{"iopub.execute_input":"2024-10-05T15:37:33.230509Z","iopub.status.busy":"2024-10-05T15:37:33.230052Z","iopub.status.idle":"2024-10-05T15:38:29.725182Z","shell.execute_reply":"2024-10-05T15:38:29.723867Z","shell.execute_reply.started":"2024-10-05T15:37:33.230473Z"}},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82cd97d4e7474a37a9bece3598ffaccf","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"28f38c9fc69d4357960b8c74b73e14a9","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"58d8ed20ce724653a344781851e34f36","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f6613014fa84ee2b9bfb3df7cb2b6fa","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"796d36265b0e4ca3a7d150e8388eaac2","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1464bdeb7bb841ba9f0955439ff4339a","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a4b0d1fa824413cbcbad2f5da8a13af","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88dacd107b4d488e8e61fa8f0c2a8195","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e93fe61e3a14f47a90849e1e408fb5f","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n\n  warnings.warn(\n"},{"name":"stdout","output_type":"stream","text":"Original Article:\n\n Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.Â 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\n\n\n\nGenerated Summary:\n\n U.S consumer advisory group set up by Department of Transportation said that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased.\n"}]},{"cell_type":"markdown","source":"C) Extractive Summarization Using Bert and Spacy","metadata":{}},{"cell_type":"code","source":"# 1. Install Required Libraries:\n# pip install spacy torch transformers\n# python -m spacy download en_core_web_md\n\nimport spacy\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\n# 2. Import Required Libraries:\nnlp = spacy.load('en_core_web_md')  # Load SpaCy model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Load BERT tokenizer\nmodel = BertModel.from_pretrained('bert-base-uncased')  # Load BERT model\n\n# 4. Define Sample Text:\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence \nconcerned with the interactions between computers and human language, in particular how to program computers \nto process and analyze large amounts of natural language data.\n\"\"\"\n\n# 5. Preprocess and Tokenize Sentences:\ndoc = nlp(text)\nsentences = [sent.text for sent in doc.sents]  # Extract sentences using SpaCy\n\n# Tokenize sentences for BERT\ntokenized_sentences = [tokenizer.encode(sentence, return_tensors='pt', max_length=512, truncation=True) for sentence in sentences]\n\n# 6. Compute BERT Embeddings:\nwith torch.no_grad():\n    embeddings = [model(sentence)[0].mean(dim=1) for sentence in tokenized_sentences]  # Compute BERT embeddings for each sentence\n\n# 7. Compute Sentence Scores and Generate Summary:\n# Calculate sentence scores (mean of embeddings) and select the top sentences\nsentence_scores = torch.stack(embeddings).mean(dim=1)\ntop_sentence_indices = sentence_scores.argsort(descending=True)[:2]  # Select top 2 sentences for the summary\n\n# Generate summary\nsummary = ' '.join([sentences[i] for i in top_sentence_indices])\nprint(\"Summary:\")\nprint(summary)\n","metadata":{},"execution_count":null,"outputs":[]}]}